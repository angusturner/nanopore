{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Nanopore Sequencing for ML Engineers\n",
        "\n",
        "Welcome to genomics! This notebook will teach you nanopore sequencing concepts through the lens of machine learning and data science.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **What is nanopore sequencing?** (Think: streaming data with noise)\n",
        "2. **Working with biological sequences** (Think: variable-length categorical sequences)\n",
        "3. **Sequence quality and error patterns** (Think: noisy labels and systematic biases)\n",
        "4. **Genomics-specific metrics** (Think: domain-specific evaluation metrics)\n",
        "5. **How this relates to ML problems** (Think: sequence modeling, error correction, classification)\n",
        "\n",
        "## Our Dataset\n",
        "\n",
        "We're using a fantastic real-world dataset from Zenodo:\n",
        "- **E. coli K-12 MG1655** nanopore sequencing data (FASTQ format)\n",
        "- **Basecalled with Bonito** (high-quality basecaller)\n",
        "- **Oxford Nanopore MinION** sequencer data\n",
        "- **Quality scores included** - perfect for ML applications!\n",
        "\n",
        "**Why E. coli K-12 MG1655?**\n",
        "- The \"MNIST of genomics\" - most studied bacterial strain\n",
        "- Small genome (~4.6 million base pairs) - manageable for learning\n",
        "- Single circular chromosome - simple structure\n",
        "- Reference genome available from the same dataset\n",
        "- Authentic nanopore error patterns for ML training\n",
        "\n",
        "**Dataset advantages:**\n",
        "âœ… FASTQ format (sequences + quality scores)  \n",
        "âœ… Real MinION data (authentic error patterns)  \n",
        "âœ… Matched reference genome  \n",
        "âœ… High-quality basecalling (Bonito)  \n",
        "âœ… Perfect for ML error correction models  \n",
        "\n",
        "Let's dive in! ðŸ§¬\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Understanding Nanopore Sequencing\n",
        "\n",
        "**What is nanopore sequencing?**\n",
        "\n",
        "Imagine you have a tiny hole (nanopore) in a membrane. DNA molecules pass through this hole one at a time. As each nucleotide (A, T, G, C) passes through, it changes the electrical current in a characteristic way.\n",
        "\n",
        "**ML Analogy:**\n",
        "- Think of it as **streaming sensor data**\n",
        "- Each nucleotide produces a different **electrical signal**\n",
        "- You need to **decode** these signals back to DNA sequence\n",
        "- It's a **sequence-to-sequence** problem with noise\n",
        "\n",
        "**Key characteristics:**\n",
        "- **Long reads**: 1,000 - 100,000+ base pairs (vs. 150-300 for Illumina)\n",
        "- **Higher error rate**: ~10-15% vs. ~0.1% for Illumina\n",
        "- **Real-time**: Data comes off the sequencer live\n",
        "- **Single molecule**: Each read is from one DNA molecule\n",
        "\n",
        "This makes it perfect for:\n",
        "- Genome assembly (long reads span repetitive regions)\n",
        "- Structural variant detection\n",
        "- Real-time analysis (pathogen identification)\n",
        "\n",
        "But challenging for:\n",
        "- High-accuracy applications\n",
        "- Small variant detection\n",
        "- Cost-sensitive projects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from Bio import SeqIO\n",
        "import gzip\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "\n",
        "# ML libraries we'll use later\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(\"ðŸ“‚ Current working directory:\", Path.cwd())\n",
        "\n",
        "# Check if data exists\n",
        "data_file = Path(\"../data/raw/ecoli_nanopore_reads.fastq.gz\")\n",
        "if data_file.exists():\n",
        "    print(f\"âœ… Data file found: {data_file}\")\n",
        "    print(f\"   File size: {data_file.stat().st_size / (1024*1024):.1f} MB\")\n",
        "    print(\"ðŸ“Š This is FASTQ format - sequences with quality scores!\")\n",
        "else:\n",
        "    print(\"âŒ Data file not found!\")\n",
        "    print(\"   Run this first: uv run python scripts/download_data.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Loading and Exploring Sequence Data\n",
        "\n",
        "In bioinformatics, we work with several file formats:\n",
        "- **FASTA**: Just sequences (like text files)\n",
        "- **FASTQ**: Sequences + quality scores (like labeled data)\n",
        "- **SAM/BAM**: Aligned sequences (like predictions with ground truth)\n",
        "\n",
        "Our data is in FASTA format - think of it as a collection of variable-length strings where each character is one of four categories: A, T, G, C.\n",
        "\n",
        "**ML Perspective:**\n",
        "- Each read = one training sample\n",
        "- Each nucleotide = one feature/token\n",
        "- Variable length sequences (like sentences in NLP)\n",
        "- Categorical data (4 classes: A, T, G, C)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample of reads for exploration\n",
        "print(\"ðŸ§¬ Loading nanopore reads...\")\n",
        "\n",
        "# Load first 10,000 reads for interactive exploration\n",
        "reads_data = []\n",
        "max_reads = 10000  # Limit for notebook performance\n",
        "\n",
        "with gzip.open(data_file, \"rt\") as handle:\n",
        "    for i, record in enumerate(SeqIO.parse(handle, \"fastq\")):\n",
        "        if i >= max_reads:\n",
        "            break\n",
        "            \n",
        "        # FASTQ format gives us both sequence and quality scores!\n",
        "        reads_data.append({\n",
        "            'read_id': record.id,\n",
        "            'sequence': str(record.seq),\n",
        "            'length': len(record.seq),\n",
        "            'quality_scores': record.letter_annotations['phred_quality'],\n",
        "            'mean_quality': sum(record.letter_annotations['phred_quality']) / len(record.letter_annotations['phred_quality'])\n",
        "        })\n",
        "        \n",
        "        if (i + 1) % 2000 == 0:\n",
        "            print(f\"  Loaded {i + 1:,} reads...\")\n",
        "\n",
        "print(f\"âœ… Loaded {len(reads_data):,} reads for analysis\")\n",
        "\n",
        "# Quick peek at the data structure\n",
        "print(f\"\\nðŸ“Š Data structure:\")\n",
        "print(f\"  Type: {type(reads_data)}\")\n",
        "print(f\"  First read keys: {list(reads_data[0].keys())}\")\n",
        "print(f\"\\nðŸ” First few reads:\")\n",
        "\n",
        "for i in range(3):\n",
        "    read = reads_data[i]\n",
        "    sequence_preview = read['sequence'][:50] + \"...\" if len(read['sequence']) > 50 else read['sequence']\n",
        "    print(f\"  Read {i+1}: ID={read['read_id'][:20]}..., Length={read['length']:,}bp\")\n",
        "    print(f\"         Sequence={sequence_preview}\")\n",
        "    print(f\"         Mean Quality: {read['mean_quality']:.1f} (Phred score)\")\n",
        "    print()\n",
        "\n",
        "print(\"ðŸ“Š âœ… Quality scores available - FASTQ format with Phred scores!\")\n",
        "print(\"ðŸ’¡ Quality scores help identify reliable vs. unreliable bases\")\n",
        "print(\"   Higher scores = more confident base calls\")\n",
        "print(\"   This is crucial for ML error correction models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Understanding Read Length Distributions\n",
        "\n",
        "Read length is one of the most important characteristics of sequencing data.\n",
        "\n",
        "**Why length matters:**\n",
        "- **Assembly**: Longer reads can span repetitive regions that short reads cannot\n",
        "- **Mapping**: Longer reads map more uniquely to genomes\n",
        "- **Errors**: Longer reads have more errors in absolute terms, but similar error rates\n",
        "- **Computational cost**: Longer sequences require more processing time\n",
        "\n",
        "**ML Perspective:**\n",
        "- Like having variable-length input sequences in NLP\n",
        "- Need to handle different sequence lengths in models\n",
        "- May need padding, truncation, or specialized architectures (RNNs, Transformers)\n",
        "- Length can be a useful feature for downstream tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze read length distribution\n",
        "lengths = [read['length'] for read in reads_data]\n",
        "\n",
        "# Calculate key statistics\n",
        "def calculate_n50(lengths):\n",
        "    \"\"\"\n",
        "    N50: The length such that 50% of all bases are in reads >= this length.\n",
        "    \n",
        "    Different from median! N50 is weighted by read length.\n",
        "    Think of it as: \"Half of the sequenced DNA comes from reads this long or longer\"\n",
        "    \"\"\"\n",
        "    sorted_lengths = sorted(lengths, reverse=True)\n",
        "    total_bases = sum(sorted_lengths)\n",
        "    cumulative_bases = 0\n",
        "    \n",
        "    for length in sorted_lengths:\n",
        "        cumulative_bases += length\n",
        "        if cumulative_bases >= total_bases / 2:\n",
        "            return length\n",
        "    return 0\n",
        "\n",
        "# Basic statistics\n",
        "stats = {\n",
        "    'count': len(lengths),\n",
        "    'total_bases': sum(lengths),\n",
        "    'mean': np.mean(lengths),\n",
        "    'median': np.median(lengths),\n",
        "    'std': np.std(lengths),\n",
        "    'min': min(lengths),\n",
        "    'max': max(lengths),\n",
        "    'n50': calculate_n50(lengths)\n",
        "}\n",
        "\n",
        "print(\"ðŸ“Š READ LENGTH STATISTICS\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Total reads:      {stats['count']:,}\")\n",
        "print(f\"Total bases:      {stats['total_bases']:,} ({stats['total_bases']/1e6:.1f}M)\")\n",
        "print(f\"Mean length:      {stats['mean']:,.0f} bp\")\n",
        "print(f\"Median length:    {stats['median']:,.0f} bp\")\n",
        "print(f\"Standard dev:     {stats['std']:,.0f} bp\")\n",
        "print(f\"N50:              {stats['n50']:,.0f} bp\")\n",
        "print(f\"Range:            {stats['min']:,} - {stats['max']:,} bp\")\n",
        "\n",
        "# Percentiles (like quantiles in data science)\n",
        "percentiles = [5, 25, 75, 95]\n",
        "print(f\"\\nPercentiles:\")\n",
        "for p in percentiles:\n",
        "    value = np.percentile(lengths, p)\n",
        "    print(f\"  {p}th percentile: {value:,.0f} bp\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize read length distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Histogram of read lengths\n",
        "axes[0, 0].hist(lengths, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[0, 0].axvline(stats['mean'], color='red', linestyle='--', label=f'Mean: {stats[\"mean\"]:,.0f}')\n",
        "axes[0, 0].axvline(stats['median'], color='orange', linestyle='--', label=f'Median: {stats[\"median\"]:,.0f}')\n",
        "axes[0, 0].axvline(stats['n50'], color='green', linestyle='--', label=f'N50: {stats[\"n50\"]:,.0f}')\n",
        "axes[0, 0].set_xlabel('Read Length (bp)')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].set_title('Read Length Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_yscale('log')  # Log scale to see the full distribution\n",
        "\n",
        "# 2. Cumulative distribution (like CDF in statistics)\n",
        "sorted_lengths = sorted(lengths, reverse=True)\n",
        "cumulative_bases = np.cumsum(sorted_lengths)\n",
        "axes[0, 1].plot(range(len(cumulative_bases)), cumulative_bases, linewidth=2)\n",
        "axes[0, 1].axhline(stats['total_bases'] / 2, color='red', linestyle='--', \n",
        "                   label='50% of bases')\n",
        "axes[0, 1].set_xlabel('Read Rank (longest to shortest)')\n",
        "axes[0, 1].set_ylabel('Cumulative Bases')\n",
        "axes[0, 1].set_title('Cumulative Base Distribution')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Box plot with quartiles\n",
        "bp = axes[1, 0].boxplot([lengths], patch_artist=True, \n",
        "                        boxprops=dict(facecolor='lightblue'))\n",
        "axes[1, 0].set_ylabel('Read Length (bp)')\n",
        "axes[1, 0].set_title('Read Length Box Plot')\n",
        "axes[1, 0].set_xticks([1])\n",
        "axes[1, 0].set_xticklabels(['All Reads'])\n",
        "\n",
        "# Add text annotations\n",
        "quartiles = np.percentile(lengths, [25, 50, 75])\n",
        "axes[1, 0].text(1.3, quartiles[1], f'Median: {quartiles[1]:,.0f}', \n",
        "                verticalalignment='center')\n",
        "\n",
        "# 4. Length bins (like creating features for ML)\n",
        "length_bins = [0, 1000, 5000, 10000, 20000, 50000, float('inf')]\n",
        "length_labels = ['<1kb', '1-5kb', '5-10kb', '10-20kb', '20-50kb', '>50kb']\n",
        "\n",
        "# Bin the data\n",
        "binned_counts = []\n",
        "for i in range(len(length_bins)-1):\n",
        "    count = sum(1 for l in lengths if length_bins[i] <= l < length_bins[i+1])\n",
        "    binned_counts.append(count)\n",
        "\n",
        "# Plot binned data\n",
        "bars = axes[1, 1].bar(length_labels, binned_counts, \n",
        "                      color=['#ff9999', '#66b3ff', '#99ff99', '#ffcc99', '#ff99cc', '#c2c2f0'])\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].set_title('Read Length Bins')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Add count labels on bars\n",
        "for bar, count in zip(bars, binned_counts):\n",
        "    height = bar.get_height()\n",
        "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + max(binned_counts)*0.01,\n",
        "                    f'{count:,}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ ML Engineering Observations:\")\n",
        "print(\"â€¢ Wide distribution suggests you'll need to handle variable sequence lengths\")\n",
        "print(\"â€¢ Long tail distribution - most reads are short, few are very long\")\n",
        "print(\"â€¢ This is similar to document length distributions in NLP\")\n",
        "print(\"â€¢ Consider length-based sampling or stratification for training\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Nucleotide Composition and GC Content\n",
        "\n",
        "DNA is made of four nucleotides: A (Adenine), T (Thymine), G (Guanine), C (Cytosine)\n",
        "\n",
        "**Key concepts:**\n",
        "- **Base pairing**: A pairs with T, G pairs with C (in double-stranded DNA)\n",
        "- **GC content**: Percentage of bases that are G or C\n",
        "- **GC bias**: Deviation from expected 50% (important for sequencing quality)\n",
        "\n",
        "**Why GC content matters:**\n",
        "- **Stability**: G-C pairs have 3 hydrogen bonds vs 2 for A-T (stronger)\n",
        "- **Melting temperature**: Higher GC = higher temperature needed to separate strands\n",
        "- **Sequencing bias**: Some sequencers struggle with very high or low GC regions\n",
        "- **Species identification**: Different organisms have characteristic GC contents\n",
        "\n",
        "**ML Perspective:**\n",
        "- Like analyzing character frequencies in text\n",
        "- GC content can be a useful feature for classification\n",
        "- Compositional bias can affect model training\n",
        "- Similar to analyzing word frequencies or n-gram distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze nucleotide composition\n",
        "print(\"ðŸ§® Analyzing nucleotide composition...\")\n",
        "\n",
        "# Count all nucleotides across all reads\n",
        "all_nucleotides = Counter()\n",
        "gc_contents = []\n",
        "read_compositions = []\n",
        "\n",
        "for read in reads_data:\n",
        "    sequence = read['sequence'].upper()\n",
        "    \n",
        "    # Count nucleotides in this read\n",
        "    read_counts = Counter(sequence)\n",
        "    all_nucleotides.update(read_counts)\n",
        "    \n",
        "    # Calculate GC content for this read\n",
        "    total_bases = sum(count for base, count in read_counts.items() if base in 'ATGC')\n",
        "    if total_bases > 0:\n",
        "        gc_count = read_counts.get('G', 0) + read_counts.get('C', 0)\n",
        "        gc_content = (gc_count / total_bases) * 100\n",
        "        gc_contents.append(gc_content)\n",
        "        \n",
        "        # Store composition for this read\n",
        "        composition = {\n",
        "            'A': (read_counts.get('A', 0) / total_bases) * 100,\n",
        "            'T': (read_counts.get('T', 0) / total_bases) * 100,\n",
        "            'G': (read_counts.get('G', 0) / total_bases) * 100,\n",
        "            'C': (read_counts.get('C', 0) / total_bases) * 100,\n",
        "            'GC': gc_content\n",
        "        }\n",
        "        read_compositions.append(composition)\n",
        "\n",
        "# Overall composition\n",
        "total_valid_bases = sum(all_nucleotides[base] for base in 'ATGC')\n",
        "overall_composition = {\n",
        "    base: (all_nucleotides[base] / total_valid_bases) * 100 \n",
        "    for base in 'ATGC'\n",
        "}\n",
        "overall_gc = overall_composition['G'] + overall_composition['C']\n",
        "\n",
        "print(\"ðŸ“Š OVERALL NUCLEOTIDE COMPOSITION\")\n",
        "print(\"=\" * 45)\n",
        "for base in 'ATGC':\n",
        "    print(f\"{base}: {overall_composition[base]:6.2f}% ({all_nucleotides[base]:,} bases)\")\n",
        "\n",
        "print(f\"\\nGC content: {overall_gc:.2f}%\")\n",
        "print(f\"AT content: {100 - overall_gc:.2f}%\")\n",
        "\n",
        "# GC content statistics\n",
        "gc_stats = {\n",
        "    'mean': np.mean(gc_contents),\n",
        "    'median': np.median(gc_contents),\n",
        "    'std': np.std(gc_contents),\n",
        "    'min': min(gc_contents),\n",
        "    'max': max(gc_contents)\n",
        "}\n",
        "\n",
        "print(f\"\\nðŸ“Š GC CONTENT STATISTICS (per read)\")\n",
        "print(\"=\" * 45)\n",
        "for stat, value in gc_stats.items():\n",
        "    print(f\"{stat.capitalize():>8}: {value:6.2f}%\")\n",
        "\n",
        "# Check for unusual bases (sequencing errors or ambiguous calls)\n",
        "unusual_bases = {base: count for base, count in all_nucleotides.items() \n",
        "                 if base not in 'ATGC' and count > 0}\n",
        "\n",
        "if unusual_bases:\n",
        "    print(f\"\\nâš ï¸  UNUSUAL BASES DETECTED:\")\n",
        "    for base, count in unusual_bases.items():\n",
        "        pct = (count / sum(all_nucleotides.values())) * 100\n",
        "        print(f\"   {base}: {count:,} occurrences ({pct:.3f}%)\")\n",
        "        \n",
        "    print(\"   Note: 'N' usually means ambiguous/unknown base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize nucleotide composition and GC content\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 1. Overall nucleotide composition (bar chart)\n",
        "bases = ['A', 'T', 'G', 'C']\n",
        "percentages = [overall_composition[base] for base in bases]\n",
        "colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']\n",
        "\n",
        "bars = axes[0, 0].bar(bases, percentages, color=colors, edgecolor='black', alpha=0.8)\n",
        "axes[0, 0].set_ylabel('Percentage (%)')\n",
        "axes[0, 0].set_title('Overall Nucleotide Composition')\n",
        "axes[0, 0].set_ylim(0, max(percentages) * 1.1)\n",
        "\n",
        "# Add percentage labels on bars\n",
        "for bar, pct in zip(bars, percentages):\n",
        "    axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n",
        "                    f'{pct:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Expected vs observed (E. coli has ~50.8% GC content)\n",
        "expected_gc = 50.8\n",
        "axes[0, 0].axhline(y=expected_gc/2, color='red', linestyle='--', alpha=0.5, \n",
        "                   label=f'Expected G/C ({expected_gc/2:.1f}%)')\n",
        "axes[0, 0].axhline(y=(100-expected_gc)/2, color='orange', linestyle='--', alpha=0.5,\n",
        "                   label=f'Expected A/T ({(100-expected_gc)/2:.1f}%)')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 2. GC content distribution across reads\n",
        "axes[0, 1].hist(gc_contents, bins=40, edgecolor='black', alpha=0.7, color='teal')\n",
        "axes[0, 1].axvline(gc_stats['mean'], color='red', linestyle='--', \n",
        "                   label=f'Mean: {gc_stats[\"mean\"]:.1f}%')\n",
        "axes[0, 1].axvline(expected_gc, color='orange', linestyle='--', \n",
        "                   label=f'E. coli expected: {expected_gc}%')\n",
        "axes[0, 1].set_xlabel('GC Content (%)')\n",
        "axes[0, 1].set_ylabel('Number of Reads')\n",
        "axes[0, 1].set_title('GC Content Distribution Across Reads')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. GC content vs Read length (scatter plot)\n",
        "sample_idx = np.random.choice(len(read_compositions), size=min(2000, len(read_compositions)), replace=False)\n",
        "sample_gc = [gc_contents[i] for i in sample_idx]\n",
        "sample_lengths = [lengths[i] for i in sample_idx]\n",
        "\n",
        "axes[1, 0].scatter(sample_gc, sample_lengths, alpha=0.6, s=10, color='purple')\n",
        "axes[1, 0].set_xlabel('GC Content (%)')\n",
        "axes[1, 0].set_ylabel('Read Length (bp)')\n",
        "axes[1, 0].set_title('GC Content vs Read Length')\n",
        "\n",
        "# Add correlation coefficient\n",
        "correlation = np.corrcoef(sample_gc, sample_lengths)[0, 1]\n",
        "axes[1, 0].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
        "                transform=axes[1, 0].transAxes, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
        "\n",
        "# 4. Composition heatmap (showing variation across reads)\n",
        "# Sample some reads for visualization\n",
        "sample_size = min(100, len(read_compositions))\n",
        "sample_compositions = read_compositions[:sample_size]\n",
        "\n",
        "# Create matrix for heatmap\n",
        "comp_matrix = np.array([[comp[base] for base in bases] for comp in sample_compositions])\n",
        "\n",
        "im = axes[1, 1].imshow(comp_matrix, cmap='viridis', aspect='auto')\n",
        "axes[1, 1].set_xlabel('Nucleotide')\n",
        "axes[1, 1].set_ylabel(f'Read Index (sample of {sample_size})')\n",
        "axes[1, 1].set_title('Composition Heatmap Across Reads')\n",
        "axes[1, 1].set_xticks(range(len(bases)))\n",
        "axes[1, 1].set_xticklabels(bases)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=axes[1, 1])\n",
        "cbar.set_label('Percentage (%)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key Observations:\")\n",
        "print(f\"â€¢ E. coli expected GC content: ~{expected_gc}%\")\n",
        "print(f\"â€¢ Observed GC content: {overall_gc:.1f}%\")\n",
        "print(f\"â€¢ GC bias: {abs(overall_gc - expected_gc):.1f}% deviation\")\n",
        "print(f\"â€¢ GC-length correlation: {correlation:.3f}\")\n",
        "\n",
        "if abs(overall_gc - expected_gc) > 2:\n",
        "    print(\"âš ï¸  Significant GC bias detected - check data quality\")\n",
        "else:\n",
        "    print(\"âœ… GC content matches expected values - good data quality\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.5. Quality Score Analysis - FASTQ Advantage!\n",
        "\n",
        "Since our data is in FASTQ format, we have **quality scores** for each nucleotide! This is huge for ML applications.\n",
        "\n",
        "**What are quality scores?**\n",
        "- **Phred scores**: Measure confidence in each base call\n",
        "- **Scale**: Higher = better (usually 0-40+)\n",
        "- **Logarithmic**: Q20 = 1% error rate, Q30 = 0.1% error rate, Q40 = 0.01% error rate\n",
        "\n",
        "**Quality Score Formula:**\n",
        "```\n",
        "Q = -10 * log10(P_error)\n",
        "```\n",
        "Where P_error is the probability the base call is wrong.\n",
        "\n",
        "**ML Perspective:**\n",
        "- Like **confidence scores** in classification models\n",
        "- Can weight training samples by quality\n",
        "- Essential for **uncertainty quantification**\n",
        "- Perfect for building **quality-aware models**\n",
        "- Similar to attention weights or prediction confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze quality scores from our FASTQ data\n",
        "print(\"ðŸ“Š Analyzing quality scores...\")\n",
        "\n",
        "# Extract quality data from our reads\n",
        "all_qualities = []\n",
        "read_mean_qualities = []\n",
        "quality_by_position = defaultdict(list)\n",
        "\n",
        "# Sample reads for position analysis (too memory intensive otherwise)\n",
        "sample_reads = reads_data[:1000]\n",
        "\n",
        "for read in sample_reads:\n",
        "    qualities = read['quality_scores']\n",
        "    all_qualities.extend(qualities)\n",
        "    read_mean_qualities.append(read['mean_quality'])\n",
        "    \n",
        "    # Quality by position (for first 100bp to see patterns)\n",
        "    for pos, qual in enumerate(qualities[:100]):\n",
        "        quality_by_position[pos].append(qual)\n",
        "# Calculate statistics\n",
        "quality_stats = {\n",
        "    'mean': np.mean(all_qualities),\n",
        "    'median': np.median(all_qualities),\n",
        "    'std': np.std(all_qualities),\n",
        "    'min': min(all_qualities),\n",
        "    'max': max(all_qualities),\n",
        "    'q10': np.percentile(all_qualities, 10),\n",
        "    'q25': np.percentile(all_qualities, 25),\n",
        "    'q75': np.percentile(all_qualities, 75),\n",
        "    'q90': np.percentile(all_qualities, 90)\n",
        "}\n",
        "\n",
        "print(\"ðŸ“Š QUALITY SCORE STATISTICS\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"Total bases analyzed:  {len(all_qualities):,}\")\n",
        "print(f\"Mean quality:          {quality_stats['mean']:.1f}\")\n",
        "print(f\"Median quality:        {quality_stats['median']:.1f}\")\n",
        "print(f\"Standard deviation:    {quality_stats['std']:.1f}\")\n",
        "print(f\"Range:                 {quality_stats['min']} - {quality_stats['max']}\")\n",
        "print(f\"25th-75th percentile:  {quality_stats['q25']:.1f} - {quality_stats['q75']:.1f}\")\n",
        "\n",
        "# Convert to error probabilities for interpretation\n",
        "mean_error_prob = 10 ** (-quality_stats['mean'] / 10)\n",
        "median_error_prob = 10 ** (-quality_stats['median'] / 10)\n",
        "\n",
        "print(f\"\\nðŸ§® ERROR RATE INTERPRETATION:\")\n",
        "print(f\"Mean error probability:   {mean_error_prob:.4f} ({mean_error_prob*100:.2f}%)\")\n",
        "print(f\"Median error probability: {median_error_prob:.4f} ({median_error_prob*100:.2f}%)\")\n",
        "\n",
        "# Quality thresholds commonly used\n",
        "q10_count = sum(1 for q in all_qualities if q >= 10)\n",
        "q20_count = sum(1 for q in all_qualities if q >= 20)\n",
        "q30_count = sum(1 for q in all_qualities if q >= 30)\n",
        "\n",
        "print(f\"\\nðŸ“ QUALITY THRESHOLDS:\")\n",
        "print(f\"Qâ‰¥10 (â‰¤10% error):  {q10_count:,} bases ({q10_count/len(all_qualities)*100:.1f}%)\")\n",
        "print(f\"Qâ‰¥20 (â‰¤1% error):   {q20_count:,} bases ({q20_count/len(all_qualities)*100:.1f}%)\")\n",
        "print(f\"Qâ‰¥30 (â‰¤0.1% error): {q30_count:,} bases ({q30_count/len(all_qualities)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize quality scores from our FASTQ data\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Quality score distribution\n",
        "axes[0, 0].hist(all_qualities, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0, 0].axvline(quality_stats['mean'], color='red', linestyle='--', \n",
        "                   label=f'Mean: {quality_stats[\"mean\"]:.1f}')\n",
        "axes[0, 0].axvline(quality_stats['median'], color='orange', linestyle='--',\n",
        "                   label=f'Median: {quality_stats[\"median\"]:.1f}')\n",
        "\n",
        "# Add quality threshold lines\n",
        "for q, color in [(10, 'green'), (20, 'blue'), (30, 'purple')]:\n",
        "    axes[0, 0].axvline(q, color=color, linestyle=':', alpha=0.7, \n",
        "                      label=f'Q{q} threshold')\n",
        "\n",
        "axes[0, 0].set_xlabel('Quality Score (Phred)')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].set_title('Quality Score Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# 2. Quality vs Read Length\n",
        "sample_indices = np.random.choice(len(read_mean_qualities), \n",
        "                                size=min(2000, len(read_mean_qualities)), \n",
        "                                replace=False)\n",
        "sample_quals = [read_mean_qualities[i] for i in sample_indices]\n",
        "sample_lens = [lengths[i] for i in sample_indices]\n",
        "\n",
        "axes[0, 1].scatter(sample_lens, sample_quals, alpha=0.6, s=10, color='purple')\n",
        "axes[0, 1].set_xlabel('Read Length (bp)')\n",
        "axes[0, 1].set_ylabel('Mean Quality Score')\n",
        "axes[0, 1].set_title('Quality vs Read Length')\n",
        "\n",
        "# Add correlation\n",
        "qual_length_corr = np.corrcoef(sample_lens, sample_quals)[0, 1]\n",
        "axes[0, 1].text(0.05, 0.95, f'Correlation: {qual_length_corr:.3f}', \n",
        "                transform=axes[0, 1].transAxes, \n",
        "                bbox=dict(boxstyle='round', facecolor='wheat'))\n",
        "\n",
        "# 3. Quality by position (first 50bp)\n",
        "positions = sorted(quality_by_position.keys())[:50]  # First 50 positions\n",
        "mean_quals_by_pos = [np.mean(quality_by_position[pos]) for pos in positions]\n",
        "std_quals_by_pos = [np.std(quality_by_position[pos]) for pos in positions]\n",
        "\n",
        "axes[1, 0].plot(positions, mean_quals_by_pos, 'b-', linewidth=2, label='Mean Quality')\n",
        "axes[1, 0].fill_between(positions, \n",
        "                       [m-s for m,s in zip(mean_quals_by_pos, std_quals_by_pos)],\n",
        "                       [m+s for m,s in zip(mean_quals_by_pos, std_quals_by_pos)],\n",
        "                       alpha=0.3)\n",
        "axes[1, 0].set_xlabel('Position in Read')\n",
        "axes[1, 0].set_ylabel('Quality Score')\n",
        "axes[1, 0].set_title('Quality Score by Position')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].axhline(y=20, color='red', linestyle='--', alpha=0.7, label='Q20')\n",
        "axes[1, 0].axhline(y=30, color='green', linestyle='--', alpha=0.7, label='Q30')\n",
        "\n",
        "# 4. Error rate visualization\n",
        "error_rates = [10 ** (-q / 10) for q in all_qualities]\n",
        "axes[1, 1].hist(error_rates, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
        "axes[1, 1].set_xlabel('Error Probability')\n",
        "axes[1, 1].set_ylabel('Count')\n",
        "axes[1, 1].set_title('Error Rate Distribution')\n",
        "axes[1, 1].set_xscale('log')\n",
        "axes[1, 1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ’¡ Quality Analysis Insights:\")\n",
        "print(f\"â€¢ Mean quality: {quality_stats['mean']:.1f} (error rate: {mean_error_prob*100:.2f}%)\")\n",
        "print(f\"â€¢ Quality-length correlation: {qual_length_corr:.3f}\")\n",
        "print(f\"â€¢ {q20_count/len(all_qualities)*100:.1f}% of bases are Qâ‰¥20 (high quality)\")\n",
        "print(f\"â€¢ This quality information is gold for ML error correction!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Homopolymer Analysis - The Nanopore Challenge\n",
        "\n",
        "**What are homopolymers?**\n",
        "Runs of identical bases: AAAAA, TTTTT, GGGGG, CCCCC\n",
        "\n",
        "**Why do homopolymers matter for nanopore sequencing?**\n",
        "- **Major error source**: Nanopore sequencers struggle to count identical bases accurately\n",
        "- **Length ambiguity**: Is it AAAA or AAAAA? Hard to tell from electrical signal\n",
        "- **ML challenge**: Like trying to count repeated words in noisy speech\n",
        "\n",
        "**Real-world impact:**\n",
        "- Gene disruption if homopolymer length is wrong\n",
        "- Assembly errors in repetitive regions\n",
        "- False positive/negative variant calls\n",
        "\n",
        "**ML Perspective:**\n",
        "- Similar to sequence alignment problems\n",
        "- Could use attention mechanisms to focus on context\n",
        "- Error correction models need to handle variable-length insertions/deletions\n",
        "- Classic sequence labeling problem with systematic noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze homopolymer runs\n",
        "import re\n",
        "\n",
        "def find_homopolymers(sequence, min_length=4):\n",
        "    \"\"\"Find all homopolymer runs in a sequence\"\"\"\n",
        "    homopolymers = []\n",
        "    for base in 'ATGC':\n",
        "        pattern = f'{base}{{{min_length},}}'  # e.g., 'A{4,}' matches AAAA+\n",
        "        for match in re.finditer(pattern, sequence):\n",
        "            homopolymers.append({\n",
        "                'base': base,\n",
        "                'length': match.end() - match.start(),\n",
        "                'position': match.start()\n",
        "            })\n",
        "    return homopolymers\n",
        "\n",
        "print(\"ðŸ”„ Analyzing homopolymers...\")\n",
        "\n",
        "# Analyze homopolymers in sample of reads\n",
        "homopolymer_data = {'A': [], 'T': [], 'G': [], 'C': []}\n",
        "reads_with_homopolymers = 0\n",
        "total_homopolymers = 0\n",
        "\n",
        "# Use subset for performance\n",
        "sample_reads = reads_data[:2000]\n",
        "\n",
        "for read in sample_reads:\n",
        "    homopolymers = find_homopolymers(read['sequence'])\n",
        "    \n",
        "    if homopolymers:\n",
        "        reads_with_homopolymers += 1\n",
        "        total_homopolymers += len(homopolymers)\n",
        "        \n",
        "        for hp in homopolymers:\n",
        "            homopolymer_data[hp['base']].append(hp['length'])\n",
        "\n",
        "print(f\"ðŸ“Š HOMOPOLYMER ANALYSIS (sample of {len(sample_reads):,} reads)\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"Reads with homopolymers: {reads_with_homopolymers:,} ({reads_with_homopolymers/len(sample_reads)*100:.1f}%)\")\n",
        "print(f\"Total homopolymers found: {total_homopolymers:,}\")\n",
        "\n",
        "print(f\"\\nPer-base statistics:\")\n",
        "for base in 'ATGC':\n",
        "    lengths = homopolymer_data[base]\n",
        "    if lengths:\n",
        "        print(f\"  {base}: {len(lengths):4,} runs, \"\n",
        "              f\"avg length: {np.mean(lengths):.1f}, \"\n",
        "              f\"max length: {max(lengths):2d}\")\n",
        "    else:\n",
        "        print(f\"  {base}:    0 runs\")\n",
        "\n",
        "# Visualize homopolymer distributions\n",
        "if total_homopolymers > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "    bases = ['A', 'T', 'G', 'C']\n",
        "    colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4']\n",
        "    \n",
        "    for i, (base, color) in enumerate(zip(bases, colors)):\n",
        "        ax = axes[i // 2, i % 2]\n",
        "        lengths = homopolymer_data[base]\n",
        "        \n",
        "        if lengths:\n",
        "            # Histogram of homopolymer lengths\n",
        "            ax.hist(lengths, bins=range(4, max(lengths)+2), \n",
        "                   color=color, alpha=0.7, edgecolor='black')\n",
        "            ax.set_xlabel('Homopolymer Length')\n",
        "            ax.set_ylabel('Count')\n",
        "            ax.set_title(f'{base} Homopolymers (n={len(lengths):,})')\n",
        "            ax.set_yscale('log')\n",
        "            \n",
        "            # Add statistics text\n",
        "            stats_text = f'Mean: {np.mean(lengths):.1f}\\nMax: {max(lengths)}'\n",
        "            ax.text(0.7, 0.9, stats_text, transform=ax.transAxes,\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, f'No {base} homopolymers\\nfound (â‰¥4bp)', \n",
        "                   ha='center', va='center', transform=ax.transAxes)\n",
        "            ax.set_title(f'{base} Homopolymers')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Combined analysis\n",
        "    all_lengths = []\n",
        "    base_labels = []\n",
        "    for base in bases:\n",
        "        all_lengths.extend(homopolymer_data[base])\n",
        "        base_labels.extend([base] * len(homopolymer_data[base]))\n",
        "    \n",
        "    if all_lengths:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        \n",
        "        # Box plot comparison\n",
        "        plt.subplot(1, 2, 1)\n",
        "        bp_data = [homopolymer_data[base] for base in bases if homopolymer_data[base]]\n",
        "        bp_labels = [base for base in bases if homopolymer_data[base]]\n",
        "        \n",
        "        if bp_data:\n",
        "            bp = plt.boxplot(bp_data, labels=bp_labels, patch_artist=True)\n",
        "            for patch, color in zip(bp['boxes'], colors[:len(bp_data)]):\n",
        "                patch.set_facecolor(color)\n",
        "                \n",
        "        plt.ylabel('Homopolymer Length')\n",
        "        plt.title('Length Distribution by Base')\n",
        "        \n",
        "        # Overall distribution\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.hist(all_lengths, bins=range(4, max(all_lengths)+2), \n",
        "                alpha=0.7, edgecolor='black', color='gray')\n",
        "        plt.xlabel('Homopolymer Length')\n",
        "        plt.ylabel('Count')\n",
        "        plt.title('All Homopolymer Lengths')\n",
        "        plt.yscale('log')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\nðŸ’¡ ML Engineering Insights:\")\n",
        "        print(f\"â€¢ {reads_with_homopolymers/len(sample_reads)*100:.1f}% of reads contain homopolymers â‰¥4bp\")\n",
        "        print(f\"â€¢ Average homopolymer length: {np.mean(all_lengths):.1f}bp\")\n",
        "        print(f\"â€¢ Longest homopolymer: {max(all_lengths)}bp\")\n",
        "        print(f\"â€¢ This represents the main error-prone regions for nanopore sequencing\")\n",
        "        print(f\"â€¢ Perfect target for ML error correction models!\")\n",
        "else:\n",
        "    print(\"No homopolymers â‰¥4bp found in sample\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ“ Summary: From Genomics to ML\n",
        "\n",
        "**Congratulations!** You've just analyzed real nanopore sequencing data. Here's what you learned:\n",
        "\n",
        "### Genomics Concepts â†’ ML Parallels\n",
        "\n",
        "| Genomics | ML/Data Science |\n",
        "|----------|----------------|\n",
        "| **Read length distribution** | Variable-length sequences (like text) |\n",
        "| **GC content** | Feature engineering from categorical data |\n",
        "| **Homopolymer errors** | Systematic noise patterns |\n",
        "| **N50 metric** | Domain-specific evaluation metrics |\n",
        "| **Nucleotide composition** | Token/character frequency analysis |\n",
        "\n",
        "### Key Insights for ML Engineers\n",
        "\n",
        "1. **Variable-length data**: Like NLP, genomics deals with sequences of different lengths\n",
        "2. **Systematic errors**: Homopolymers are predictable error sources (perfect for ML!)\n",
        "3. **Domain knowledge matters**: Understanding biology helps build better models\n",
        "4. **Quality metrics**: N50, GC content are like accuracy, F1-score for genomics\n",
        "5. **Real-world data is messy**: Even \"high-quality\" sequencing has 10-15% error rates\n",
        "\n",
        "### Next Steps & ML Project Ideas\n",
        "\n",
        "**ðŸ”¬ Data Understanding Projects:**\n",
        "- Compare different organisms' GC content\n",
        "- Analyze error patterns in more depth\n",
        "- Build quality control pipelines\n",
        "\n",
        "**ðŸ¤– ML Model Projects:**\n",
        "- **Quality-aware error correction**: Use quality scores to weight corrections\n",
        "- **Homopolymer length correction**: Predict true length from context + quality\n",
        "- **Base calling improvement**: Train models to improve basecalling accuracy  \n",
        "- **Quality score recalibration**: Improve quality score accuracy\n",
        "- **Species classification**: Identify organism from read composition\n",
        "- **Structural variant detection**: Find large genomic rearrangements\n",
        "\n",
        "**ðŸ“Š Analysis Extensions:**\n",
        "- Align reads to reference genome (add mapping data)\n",
        "- Compare nanopore vs. Illumina sequencing\n",
        "- Real-time analysis simulation\n",
        "\n",
        "### Resources for Going Deeper\n",
        "\n",
        "- **BioPython tutorials**: https://biopython.org/wiki/Category:Tutorial\n",
        "- **Oxford Nanopore Community**: https://community.nanoporetech.com/\n",
        "- **Genomics ML papers**: Start with error correction and basecalling models\n",
        "\n",
        "---\n",
        "\n",
        "**You're now ready to tackle genomics ML problems!** The biological concepts you've learned here form the foundation for understanding why certain ML approaches work well in genomics and others don't.\n",
        "\n",
        "Happy coding! ðŸ§¬ðŸ¤–\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
